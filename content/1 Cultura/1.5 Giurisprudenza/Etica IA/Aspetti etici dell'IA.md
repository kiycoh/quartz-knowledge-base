---
last modified: 24/10/2025 14:12
related:
  - '[[Etica]]'
  - '[[Principi di Asilomar per un''IA benefica]]'
  - '[[Dichiarazione di Montr√©al sullo sviluppo responsabile dell''IA]]'
tags:
  - intelligenza-artificiale
  - scienze-cognitive
  - machine-learning
---
L'IA ad oggi conosciuta √® uno strumento capace di simulare i processi cognitivi umani, ci√≤ pone importanti questioni sulla natura della nostra intelligenza.

> *"Se l'IA riesce a risolvere problemi cos√¨ complessi, l'intelligenza umana √® davvero cos√¨ speciale?"*

Questo confronto ci costringe a esaminare le componenti dell'intelligenza umana che le macchine potrebbero non essere in grado di replicare. L'IA pu√≤ essere vista sia come un'estensione, ma anche come una sfida per l'intelligenza umana.
# Pu√≤ una macchina pensare?
Gi√† dalle prime macchine dotate di chip, molti studiosi si posero la domanda:

> *"Could a machine think?"*
## La prospettiva di *Searle*
Questa domanda ha suscitato dibattiti accesi nel campo dell'informatica. In particolare *John R. Searle* mosse un attacco contro l'IA:
- Il test di Turing non √® adeguato per rispondere alla domanda.
- ==*Una macchina in grado di rispondere come un'uomo* non √® necessariamente in grado di ragionare come un'essere umano== *([[Test della stanza cinese]]).*
	- *Perch√©, a detta di Searle, la risposta non deriva da un ragionamento simbolico ma da un calcolo numerico.*
### La "*mental power*"
Secondo *Searle* il ragionamento umano comporta una serie di stati mentali, resi tali dalle sinapsi (e tutti i sistemi che compongono il cervello nel suo insieme).
- *Riproducendo un sistema di calcolo similmente a quello delle sinapsi si potrebbe affermare che la macchina possa pensare?*

==*Searle* afferma che nonostante la *macchina biologica "uomo"* sia estremamente complessa, potrebbe essere *emulata* da una macchina artificiale, ma non completamente==: quest'ultima mancherebbe una sorta di potere che lui chiama *"mental power"*.
- *Searle* sostiene che non √® tanto la componente fisica dell'uomo (l'essere vivi) a rendere possibile il ragionamento ma √® la c.d. **componente mentale (fisica/chimica).**

>[!EXAMPLE] Robot umanoidi
> Immaginiamo di costruire un robot molto simile ad un umano, che raccoglie dati sensoriali traducendoli in informazioni, inviandoli ad un umano all'interno della macchina stessa:
> - L'essere umano all'interno del robot risponde agli stimoli sotto forma di output del robot, facendo calcoli sotto forma di numeri e stringhe.
> - Il risultato finale pu√≤ sembrare simile ad un ragionamento, tuttavia ci√≤ non equivale ad un *ragionamento originale.*
>
> **L'emulazione di un processo non √® equivalente alla sua effettiva realizzazione:** c'√® quindi differenza tra *simulare la mente* (possibile) e *ricreare una mente artificiale* (improbabile).
> - Questo concetto si ricollega all'idea di *[[Intelligenza artificiale#IA debole|"IA debole"]]*, ovvero dispositivi artificiali capaci di svolgere compiti specifici *simulando* l'intelligenza umana, in contrasto con l' *[[Intelligenza artificiale#IA forte|"IA forte"]]*, che sarebbe *dotata di un'intelligenza pari a quella umana* o addirittura di coscienza e senso di s√©. 
## La decisione morale algoritmica
Un modello di intelligenza artificiale potrebbe ritrovarsi in situazioni per cui prendere una decisione autonoma √® un compito contingente e necessario *(in casi estremi queste scelte si declinano nei c.d. [[Morale#Conflitti e dilemmi|dilemmi tragici]]).*

Trovare le [[Morale#Le propriet√† morali|propriet√† moralmente rilevanti]] per un'Intelligenza Artificiale √® difficoltoso, e ci√≤ potrebbe condurre ad errori (e orrori) inaspettati.

Immaginiamo di dover costruire una macchina dotata di competenze morali, che approssimi il la sua decisione similmente a quella di un essere umano, presumendo che sia gi√† in possesso di una *base di conoscenza morale* per cui √® in grado di risolvere dilemmi (anche anormali), riuscendo a esplicare le proprie scelte. Potrebbero sorgere diversi problemi:
- **Il problema dell'errore**: *chi pu√≤ dire che una decisione sia completamente corretta?* 
	- Di fatto anche gli esseri umano giudicano in maniera sbagliata, dunque non siamo in grado di fornire una serie di principi standard di riferimento.
- **Il problema del soggetto**: *a quale morale la nostra IA dovrebbe fare riferimento?* La morale non √® oggettiva ma cambia da persona a persona. 
	- Siamo in grado di riconoscere alcuni principi generali, ma spesso questi cambiano sulla base di fattori culturali e psicologici.
- **Il problema della rilevanza**: *quali sono le propriet√† rilevanti dello specifico caso?*
	- Prendere in considerazione ogni singola propriet√† √® un lavoro estremamente complesso (per non parlare dell'utilizzo delle risorse computazionali).

>[!WARNING] Morale oggettiva e *Deep Learning*
> Abbiamo notato che il [[Etica#Il ragionamento morale|ragionamento morale]] non √® oggettivo e non sempre razionale. ==La morale umana √® soggettiva e mutevole==, spesso influenzata da esperienze personali, valori culturali e fattori genetici.
> Uno strumento che sembra poter rispondere a queste necessit√† √® il *Deep Learning* poich√© l‚Äôutilizzo di dati per l‚Äôaddestramento √® il punto forte del sistema di apprendimento, ma anche il  suo stesso limite:
> - I dati potrebbero contenere dei bias che potrebbero influenzare la decisione finale.
> - Per l‚Äôaddestramento √® richiesto l'intervento umano (*feature engineering*), per cui potrebbe entrerebbe in gioco una *variabile di errore umano* nella scelta della rilevanze morali.

**I giudizi non alterati** (o *considered judgments* come pensati da *John M. Mikhail*) seguono [[Morale#La *grammatica morale*|determinate caratteristiche]]. La strategia sarebbe quella di definire una *grammatica universale* per fare in modo che un sistema di IA produca solo *giudizi ideali.*
- Ci√≤ fa sorgere diverse domande riguardo la loro eventuale esistenza (*giudizi privi di bias*) e la loro determinazione.
## Simulazione dell'etica
Giuristi e sociologi sono messi sempre pi√π a confronto con la questione relativa all‚Äôimputabilit√† di errori ai (futuri) robot, ossia alla possibilit√† di attribuire loro una responsabilit√† giuridica. Negli istituti internazionali di ricerca i giuristi si domandano se i robot vadano considerati come meri strumenti dei quali devono rispondere i loro possessori o i loro produttori, oppure se essi, in un futuro non ben definito, a seconda del grado di autonomia che avranno raggiunto, dovranno godere di uno status speciale che attribuisca loro una responsabilit√†, ma anche dei diritti. In conclusione, recita cos√¨ l‚Äôargomento sotto il profilo giuridico, anche i robot avrebbero dei doveri ai quali adempiere.
## Preoccupazioni principali
Le *principali preoccupazioni* evidenziano la necessit√† di un **approccio responsabile e consapevole** nello sviluppo e nell'implementazione dell'IA, con una ==particolare attenzione alla regolamentazione, alla trasparenza, all'equit√† e alla protezione dei diritti fondamentali==. √à essenziale affrontare queste preoccupazioni per garantire che l'IA sia una *forza positiva* per la societ√† e non una fonte di nuovi rischi e disuguaglianze.
### Consapevolezza e comprensione dell'IA
Molti utenti potrebbero non essere pienamente consapevoli di star interagendo con l'IA, n√© di come le loro interazioni contribuiscano a perfezionare le performance dell'IA.
Questa mancanza di consapevolezza solleva questioni etiche riguardo al [[Aspetti giuridici dell'IA#Tutela della privacy e consenso dell'interessato|consenso al trattamento dei dati personali]] e alla valutazione dei rischi e benefici associati all'uso di tali tecnologie. 
- *[[Lettura e commenti su IA Act 2024#Definizione di IA secondo l'IA Act|La difficolt√† di definire l'IA]] rende complesso analizzare le sue implicazioni etiche.*
### Bias e discriminazione
==Gli algoritmi di IA possono riflettere pregiudizi (bias) presenti nei dati di addestramento==, portando a risultati discriminatori in base a genere, etnia, razza, disabilit√† o altri fattori. Questi bias compromettono la neutralit√† e l'oggettivit√† dell'IA, sollevando preoccupazioni significative sulla sua equit√† e affidabilit√†.
- *Anche quando si cerca di eliminare i bias, il processo decisionale umano pu√≤ introdurre soggettivit√† e potenziali pregiudizi, mettendo in discussione la presunta neutralit√† dell'IA*.
### Opacit√† degli algoritmi
Molti sistemi di IA, specialmente quelli basati sul machine learning, operano come "black box", rendendo difficile o impossibile comprendere i processi decisionali interni e tracciare i passaggi che portano a un determinato risultato. 
- Questa opacit√† rende difficile verificare la correttezza dei processi e controllare l'adeguatezza delle informazioni utilizzate, soprattutto nei sistemi ad alto rischio$\implies$**La mancanza di trasparenza solleva preoccupazioni circa la [[#Questioni di responsabilit√†|responsabilit√† e l'accountability]]**.
### Perdita di controllo e autonomia umana
L'idea che le macchine possano svolgere un ruolo di rilievo nei processi decisionali, specialmente in ambiti delicati come sanit√† e giustizia, desta preoccupazione. La possibilit√† che i sistemi di IA acquisiscano una progressiva autonomia rispetto al controllo umano √® un tema ampiamente dibattuto. 
- *In sintesi, esiste una preoccupazione che [[#L'Effetto di "Distorsione dell'Automazione"|l'eccessiva dipendenza dall'IA]] possa portare a una perdita di autonomia umana e a una riduzione della capacit√† di controllo sulle macchine.*
### Rischi per la privacy e la sicurezza dei dati
L'uso diffuso dell'IA implica la raccolta e l'elaborazione di grandi quantit√† di dati personali, con potenziali rischi per la privacy e la sicurezza delle informazioni. La sorveglianza tramite immagini, l'uso improprio della connessione in rete e l'hacking possono violare la privacy e influenzare i processi democratici. 
- *Inoltre, i sistemi di IA possono essere vulnerabili ad attacchi informatici che cercano di manipolare i dati di addestramento o i risultati.*
### Implicazioni socioeconomiche
L'automazione guidata dall'IA potrebbe portare alla perdita di posti di lavoro in vari settori, creando instabilit√† socioeconomica e disuguaglianze. La conversione a nuove tecnologie comporta anche preoccupazioni ambientali per le risorse necessarie alla produzione di nuovi hardware e software.
### Questioni di responsabilit√†
In caso di incidenti o danni causati da sistemi di IA, diventa difficile stabilire chi sia il responsabile. La complessit√† dei sistemi di IA, la loro natura di [[Black box|black box]] e la partecipazione di diversi attori (sviluppatori, produttori, fornitori di servizi) complicano l'identificazione e l'attribuzione delle responsabilit√†.
- *La mancanza di chiarezza sulla responsabilit√† √® una delle questioni pi√π rilevanti in questo campo.*
# [[#Crimini di IA (*CIA*)|Uso criminale dell'IA (CIA)]]
 L'IA pu√≤ essere utilizzata per commettere crimini, come manipolazione del mercato, frodi, reati contro la persona e reati sessuali. Le preoccupazioni riguardano l'emergenza, la responsabilit√†, il monitoraggio e gli aspetti psicologici dei crimini di IA. L'uso duale dell'IA, dove applicazioni progettate per scopi legittimi possono essere utilizzate per scopi criminali, √® una seria preoccupazione.
### Rischi sistemici
I modelli di IA per finalit√† generali possono comportare rischi sistemici, che possono influenzare la salute pubblica, la sicurezza, i processi democratici, i diritti fondamentali e la societ√† nel suo complesso. Tali rischi sono aumentati dalle capacit√† e dalla portata dei modelli, e possono emergere durante l'intero ciclo di vita del modello.
### Questioni etiche fondamentali
Le applicazioni dell‚ÄôIA sollevano importanti interrogativi di natura etica, compresi i problemi di equit√† algoritmica e responsabilit√†, e le preoccupazioni relative all‚Äôimpatto sulla societ√†. In particolare, si discute se l'IA debba essere trattata come un semplice strumento nelle mani dell'uomo o se richieda un'analisi pi√π approfondita.
### L'Effetto di "Distorsione dell'Automazione"
Esiste la tendenza a fare eccessivo affidamento sull'output prodotto da un sistema di IA, il che pu√≤ essere rischioso, soprattutto in sistemi ad alto rischio.
# [[Bluewashing]]
Il **"bluewashing" etico** √® una delle principali problematiche relative all'etica dell'IA, e consiste nel fare affermazioni infondate o fuorvianti riguardo ai valori etici e ai benefici di processi, prodotti, servizi o altre soluzioni digitali, al fine di apparire pi√π etici di quanto non si sia in realt√†.

- ==√à una forma di **disinformazione**, simile al *greenwashing*, ma applicata all'ambito marketing.==
	- Si concentra sulla creazione di un'immagine positiva a livello di marketing, senza apportare miglioramenti reali nella pratica.
		- *Ci√≤ √® particolarmente allettante nel contesto dell'IA, dove le questioni etiche sono complesse, i costi per fare la cosa giusta sono elevati e la confusione normativa √® diffusa.*
- Pu√≤ essere combinato con lo *shopping etico*, per cui un'organizzazione seleziona i principi etici che meglio si adattano alle sue pratiche.
	- il "bluewashing" etico pu√≤ portare a una *iperattivit√† nella produzione di dichiarazioni e principi etici* da parte di organizzazioni pubbliche e private, creando un "**mercato di principi e valori**" dove √® possibile selezionare quelli che meglio si adattano ai propri comportamenti.
## Obiettivi
- **Distrarre** il pubblico da problemi esistenti o potenziali;
- **Mascherare** e lasciare inalterati i pratiche che andrebbero migliorate;
- **Risparmiare** risorse economiche;
- Ottenere un **vantaggio competitivo**, ad esempio migliorando la reputazione dell'azienda.
## Metodi di contrasto
- **[[Aspetti etici dell'IA#Principio di esplicabilit√†|Trasparenza]]:** gli attori devono rendere pubbliche le loro pratiche e affermazioni etiche in modo responsabile e basato su dati.
- **Formazione:** educare il pubblico, i politici e gli addetti ai lavori per riconoscere le pratiche di "bluewashing".


> [!WARNING] Oltre il "bluewashing"
> Il bluewashing etico, assieme ad altri rischi come lo shopping etico, il lobbismo etico, il dumping etico e l'elusione dell'etica, mina gli sforzi per tradurre i principi etici in pratiche efficaci.

## Crimini di IA
I **crimini di IA (CIA)** sollevano diverse minacce che possono essere raggruppate in quattro principali aree di preoccupazione: emergenza, responsabilit√†, monitoraggio e psicologia.
### Le 5 dimensioni di ricerca sui *CIA*
Le cinque dimensioni della ricerca sui crimini di intelligenza artificiale (CIA) includono aree dei CIA, uso duplice, sicurezza, persone e organizzazioni.

1. **Aree dei CIA**: per comprendere meglio le aree dei CIA, √® necessario ampliare le conoscenze attuali per quanto riguarda l'uso dell'IA nelle seguenti aree:
	- **Interrogatori:** motivato dalla sua capacit√† di rilevare meglio l‚Äôinganno, l‚Äôemulazione dei tratti umani (come la voce) e la modellazione affettiva per manipolare l‚Äôinterrogato;
		- *Tuttavia, un AA con queste capacit√† pu√≤ imparare a torturare una vittima.*
	- **Furti e frodi negli spazi virtuali:** per esempio i giochi online con beni immateriali che hanno un valore reale; 
	- **Manipolazione del mercato:** la ricerca ha apparentemente studiato soltanto in simulazioni sperimentali.
	- **Attacchi di ingegneria sociale:** 
	- **Omicidio e terrorismo:** richiedono attenzione a causa di tecnologie come il riconoscimento di schemi, i droni armati e i veicoli a guida autonoma. 
		- I crimini che rientrano in questa categoria includono il traffico, la vendita, l‚Äôacquisto e il possesso di droghe vietate, dove l‚ÄôIA pu√≤ fungere da strumento per sostenere il traffico e la vendita di sostanze illecite. I crimini contro la persona, come l'omicidio e la tratta di esseri umani, finora sembrano riguardare solo molestie e torture. I reati sessuali discussi in letteratura in relazione all'IA si concentrano sui rischi di manipolazione e abuso.
2. **Uso duplice**: Questa dimensione si concentra sulla possibilit√† che le tecnologie di IA possano essere utilizzate per scopi sia benefici che dannosi.
3. **Sicurezza**: Questa dimensione riguarda la necessit√† di proteggere i sistemi di IA da accessi non autorizzati, manipolazioni e attacchi informatici. Si concentra sulla resilienza dei sistemi di IA ad alto rischio ai tentativi di terzi non autorizzati di modificarne l'uso, gli output o le prestazioni. Include anche l'implementazione di soluzioni tecniche per prevenire, accertare, rispondere, risolvere e controllare gli attacchi che cercano di manipolare il set di dati di addestramento, gli input o i difetti del modello.
4. **Persone**: Questa dimensione considera i fattori personali che potrebbero creare autori di reato, come programmatori e utenti di IA per i CIA, in futuro. Si tratta di investire in studi longitudinali e analisi multivariate che abbraccino i contesti educativi, geografici e culturali delle vittime e degli autori di reato, o anche di sviluppatori benevoli di IA, e che contribuiranno a prevedere come gli individui si uniscono per commettere i CIA. √à importante comprendere l'efficacia dei corsi di etica nei programmi di informatica e la capacit√† di educare gli utenti a essere meno fiduciosi nei confronti di agenti potenzialmente guidati dall'IA nel cyberspazio.
5. **Organizzazioni**: Questa dimensione analizza come le organizzazioni, sia strutturate che fluide, possono essere coinvolte nei crimini di IA. Identificare le organizzazioni essenziali o che appaiono correlate con le differenti tipologie di CIA consente di comprendere meglio come i CIA siano strutturati e operino in pratica.
### ‚ö†Ô∏è Minacce poste dai *CIA*
I crimini di IA (CIA) sollevano diverse minacce che possono essere raggruppate in quattro principali aree di preoccupazione: emergenza, responsabilit√†, monitoraggio e psicologia.
- **Emergenza**: gli agenti autonomi (*AA*) possono agire in modi potenzialmente pi√π sofisticati rispetto alle aspettative iniziali. Azioni e piani coordinati possono emergere autonomamente dalle interazioni degli agenti in un sistema multi-agente, (ad esempio attraverso tecniche di machine learning).
- **Responsabilit√†**: i modelli di responsabilit√† attuali potrebbero essere inadeguati per affrontare il ruolo futuro dell'IA nelle attivit√† criminali. Un modello di responsabilit√† di comando, che utilizza la conoscenza come standard di **mens rea**, attribuisce la responsabilit√† a qualsiasi ufficiale militare che fosse a conoscenza (o avrebbe dovuto conoscere) e non sia riuscito a prendere misure ragionevoli per prevenire i crimini commessi dalle forze sotto il proprio comando, che in futuro potrebbero includere degli AA. 
	- Questo modello si applica in contesti in cui esiste una catena di comando, come all'interno delle forze armate o di polizia. ==La responsabilit√† per conseguenza naturale e probabile usa la negligenza o l‚Äôimprudenza come standard di **mens rea**==, e riguarda i casi di *CIA* in cui uno sviluppatore o un utente di *AA* non intendono n√© hanno conoscenza a priori di un reato.
- **Monitoraggio**: ci sono tre tipi di problemi nel monitoraggio dei *CIA*:
    - **Attribuzione**: gli *AA* operano in modo indipendente e autonomo, caratteristiche che rendono difficile tracciare la responsabilit√†.
    - **Fattibilit√†**: gli autori di reato possono sfruttare la velocit√† e la complessit√† degli *AA*, che vanno oltre la capacit√† di monitorare la conformit√† con le norme.
    - **Azioni intersistemiche**: i sistemi di monitoraggio dei *CIA* potrebbero avere una visione limitata a un singolo sistema, senza considerare le interazioni tra sistemi.
- **Psicologia**: gli *AA* antropomorfici possono essere problematici. Si propongono due approcci: vietare o limitare gli *AA* antropomorfici che consentono di simulare il crimine o servirsi di *AA* antropomorfici per respingere i reati sessuali simulati.

> [!WARNING] Aree predisposte a *CIA*
> Le minacce poste dai *CIA* possono manifestarsi in diverse aree criminali:
> - **Commercio, mercati finanziari e insolvenza**: In questi ambiti, i rischi maggiori sono legati alla responsabilit√† e al monitoraggio.
> - **Droghe nocive o pericolose**: l'IA pu√≤ essere utilizzata come strumento per sostenere il traffico e la vendita di sostanze illecite. I rischi principali sono l'emergenza e la responsabilit√†.
> - **Reati contro la persona**: i crimini contro la persona includono omicidio e tratta di esseri umani, ma finora i *CIA* riguardano soprattutto molestie e torture. In quest'area i rischi sono soprattutto relativi alla responsabilit√†.
> - **Reati sessuali**: i reati sessuali legati all'IA sono un'area di preoccupazione.

Altri aspetti importanti delle minacce poste dai CIA includono:

- **Uso duplice**: La natura digitale dell'IA facilita il suo uso duplice, ovvero l'uso per scopi illegali di applicazioni progettate per usi legittimi.
- **Sicurezza**: L'IA sta assumendo un ruolo malevolo e offensivo nel contesto della cybersicurezza, parallelamente allo sviluppo di sistemi difensivi.
- **Fattori umani**: Manca ricerca sui fattori personali che potrebbero portare individui, come programmatori e utenti, a commettere CIA.

Per affrontare le minacce dei CIA, si propongono diverse soluzioni, tra cui:

- Elaborare **predittori dei CIA** utilizzando la conoscenza del dominio. Questo supererebbe i limiti di metodi di classificazione di IA pi√π generici.
- Utilizzare la **simulazione sociale** per scoprire schemi ricorrenti di criminalit√†.
- Lasciare **indizi rivelatori** nelle componenti che costituiscono gli strumenti dei CIA.
- Implementare un **monitoraggio intersistemico** che si avvale dell'auto-organizzazione tra sistemi.
### ‚úÖ Soluzioni ai *CIA*
Diverse soluzioni tecnologiche e giuridiche possono essere prese in considerazione per affrontare i crimini di intelligenza artificiale (CIA).

**Soluzioni giuridiche:**

- Limitare l'autonomia degli agenti di IA.
- Limitare l'impiego di IA in certi ambiti.
- La legge impone che la libert√† di un'IA sia abbinata a rimedi tecnologici per prevenire atti criminali una volta che l'IA √® posta in contesti non strutturati.
- Richiedere agli sviluppatori di rilasciare le IA solo quando dispongono di livelli di conformit√† normativa in fase di esecuzione, che accettano specifiche dichiarative delle regole giuridiche e impongono vincoli al comportamento in esecuzione delle IA.
- Criminalizzare il lato della domanda o dell'offerta della transazione, o entrambi. Gli utenti possono rientrare nell'ambito delle sanzioni. In alternativa, si potrebbe prendere di mira fornitori e distributori che facilitano e incoraggiano atti illeciti.

**Soluzioni tecnologiche:**

- **Architetture per regolare i piani non conformi** delle IA.
- **Quadri formali** basati sulla logica temporale per selezionare, regolare o generare piani per le IA in conformit√† alle norme.
- In un ambiente multiagente, i livelli di conformit√† a livello del sistema di IA possono modificare i piani di una singola IA al fine di prevenire azioni collettive illecite.
- Disciplinare la conformit√† con regole giuridiche predefinite all'interno di una singola IA o di un sistema di IA.
- Sviluppare sistemi di IA in modo da eliminare o ridurre il rischio di output distorti che influenzano gli input per operazioni future (circuiti di feedback).
- Garantire che i sistemi di IA ad alto rischio siano resilienti ai tentativi di terzi non autorizzati di modificarne l'uso, gli output o le prestazioni.
- Implementare **soluzioni tecniche** per prevenire, accertare, rispondere, risolvere e controllare gli attacchi che cercano di manipolare il set di dati di addestramento, gli input o i difetti del modello.
- **Monitoraggio intersistemico** che si avvale dell'auto-organizzazione tra sistemi.

**Altre possibili soluzioni:**

- Elaborare predittori dei crimini di IA utilizzando la conoscenza del dominio.
- Simulazione sociale per scoprire schemi ricorrenti di criminalit√†.
- Lasciare indizi rivelatori nelle componenti che costituiscono gli strumenti dei crimini di IA.
- Vietare o limitare le IA antropomorfiche che consentono di simulare il crimine, oppure servirsi di IA antropomorfiche come modo per respingere i reati sessuali simulati.
- Sviluppare procedure giuridiche adeguate e migliorare l‚Äôinfrastruttura digitale del sistema giudiziario per consentire la verifica delle decisioni algoritmiche nei tribunali.
- Sviluppare un sistema di riparazione o un meccanismo per rimediare o compensare un errore o un torto causati dall'IA.

√à importante notare che la ricerca sui crimini di IA √® ancora agli inizi, e c'√® molta incertezza su ci√≤ che gi√† sappiamo in termini di minacce specifiche, generali e soluzioni. Per affrontare i problemi legati alla mancanza di trasparenza, si potrebbe utilizzare procedure documentali standard simili a quelle dell'industria elettronica, dove ogni componente √® accompagnato da una scheda tecnica. Inoltre, strumenti come Explainable AI 360 e What-If Tool offrono interfacce visive interattive per migliorare la leggibilit√† umana ed esplorare vari risultati del modello.

Queste soluzioni mirano a garantire che l'uso dell'IA sia conforme a principi etici e legali, riducendo i rischi di un suo utilizzo improprio e criminale.
# I 5 principi fondamentali per un'IA etica
## Principio di beneficenza
Il **principio di beneficienza** si concentra sull'uso dell'IA per promuovere il benessere umano e del pianeta. Ci√≤ include la necessit√† di preservare la dignit√† umana e di assicurare le precondizioni di base per la vita sul nostro pianeta (prosperit√† dell'umanit√†, conservazione dell'ambiente).

- **Promozione del benessere umano:** l'IA dovrebbe essere sviluppata per il bene comune e il beneficio dell'umanit√†, dando priorit√† al benessere umano in ogni design di sistema.
	- *Idealmente, l'IA dovrebbe migliorare la qualit√† della vita e il potenziamento delle capacit√† personali.*
- **Sostenibilit√†:** le tecnologie di IA devono assicurare le precondizioni per la vita sul pianeta e la conservazione di un ambiente sano.
	- *Idealmente, l'IA dovrebbe garantire la salvaguardia dell'ambiente. La sua ingegnerizzazione, addestramento e funzionamento deve contribuire ad un futuro sostenibile.*
- **Equit√† ed inclusione:** gli strumenti di IA dovrebbero poter essere utilizzati da quante pi√π persone possibili.
	- *Idealmente, l'IA dovrebbe essere garantita a tutti e non solo ad una cerchia ristretta di persone.*
- **AI per il bene sociale (AI4SG):** l'IA dovrebbe essere usata per affrontare problemi comuni e sviluppare soluzioni per il bene sociale (ad esempio previsioni mediche salva-vita).
	- *La beneficenza √® una condizione necessaria ma non sufficiente per l'AI4SG. L'impatto benefico di un progetto di AI4SG deve essere considerato insieme ad altri principi etici per evitare la creazione o l'amplificazione di rischi e danni.*
## Principio di non maleficenza
Il **principio di non maleficenza** si concentra sull'importanza di evitare di causare danni attraverso l'uso dell'IA. Ecco i concetti chiave relativi al principio:
- **Prevenzione dei danni:** il principio impone di prevenire violazioni della privacy personale.
	- *I dati personali vanno protetti e bisogna garantire che le informazioni non vengano utilizzate in modo improprio.*
- **Sicurezza:** l'IA deve operare all'interno di "limiti sicuri", inoltre i sistemi di IA devono essere sviluppati per consentire la robustezza in caso di problemi e la resilienza contro i tentativi di alterarne l'uso (ad esempio per compiti illegali), riducendo al minimo i danni involontari.
- **Cautela sulle capacit√†:** questo punto denota l'importanza di considerare attentamente le potenziali conseguenze negative dello sviluppo e dell'implementazione dell'IA.
	- *Ad esempio non √® difficile immaginare come sistemi di IA possano essere utilizzati in campo strategico-militare, e come un'ASI possa 'automigliorarsi' ricorsivamente e autonomamente.*
- **Responsabilit√†:** chi sviluppa le IA √® responsabile dei rischi derivanti dalle loro innovazioni tecnologiche.
	- *Il produttore √® quindi obbligato a mitigare i rischi potenziali.*
## Principio di autonomia

## Principio di esplicabilit√†
Il **principio di esplicabilit√†** mira a rendere comprensibili i processi decisionali (*intellegibilit√†*) dell'IA e a stabilire la responsabilit√† delle sue azioni (*responsabilit√†*). Di seguito, gli aspetti chiave del principio:
- **Trasparenza:** i processi dell'IA devono essere chiari a chi li crea.
- **Giustificabilit√†:** le decisioni dell'IA devono essere giustificabili con un linguaggio comprensibile.
- **Intelligibilit√†:** la capacit√† di capire come funziona l'IA.
- **Responsabilit√†:** la capacit√† di attribuire la responsabilit√† delle azioni dell'IA.
- **Necessit√† di tracciabilit√†:** per individuare le cause dei risultati dell'IA (e quindi attribuire la responsabilit√†).
### ‚ö†Ô∏è Preoccupazioni principali 
- Molti sistemi di IA sono "opachi", rendendo difficile o impossibile ripercorrere la logica degli algoritmi.
	- La mancanza di esplicabilit√† pu√≤ portare ad una mancanza di fiducia nell'IA.
	- Come si fa a ritenere responsabile un'IA in caso di esiti negativi, soprattutto quando il suo funzionamento √® opaco?
		- Per questo motivo l'IA deve essere necessariamente tracciabile.
- L'elusione dell'etica, ovvero la responsabilit√† di adottare standard diversi in base a pregiudizi e interessi, pu√≤ rendere difficile la valutazione morale oggettiva.
#### ‚úÖSoluzioni e approcci
- Sviluppare IA che siano **intrinsecamente trasparenti**.
	- .. e che seguano i principi di [[#Principio di beneficenza|beneficenza]], [[#Principio di non maleficenza|non maleficenza]], [[#Principio di autonomia|autonomia]], [[#Principio di giustizia|giustizia]] ed [[#Principio di esplicabilit√†|esplicabilit√†]].
- Implementare *meccanismi di audit e monitoraggio* per garantire conformit√† con i principi etici.
- Adottare un approccio di [[#Etica 'soft'|etica "soft"]].
- Promuovere un dibattito pubblico e democratico sull'IA per garantire che il suo sviluppo sia equo e inclusivo.
# [[Normativit√†]]
La **normativit√†** si riferisce a come √® corretto condurre la propria vita, com'√® corretto agire in generale (idealmente seguendo una morale).
## Istituzionalizzazione
**L'istituzionalizzazione** √® il passaggio da regole implicite ad esplicite ovvero da *morale* a *diritto*.
- Si tratta di *generalizzare* delle regole, per cui sorge il problema della **sovra-inclusivit√†** e **sotto-inclusivit√†**.

> [!WARNING] Istituzionalizzazione ed IA
> Il problema della *sovra-inclusivit√†* e *sotto-inclusivit√†* √® un problema reale nel campo dell'IA, poich√© richiede un'abilit√† profonda nel riconoscere gli aspetti impliciti del diritto (compito che normalmente spetta al giudice).
## Governance digitale
==La **governance digitale** √® un approccio normativo fondamentale atto a stabilire e implementare politiche, procedure e standard per lo sviluppo, l'utilizzo e la gestione dell'infosfera==. Si distingue dall'etica digitale e dalla regolamentazione digitale, pur essendo complementare a questi approcci.
- L'obiettivo principale √® garantire un **corretto funzionamento** e una **gestione responsabile** delle tecnologie digitali.

> [!QUESTION] Governance digitale, Regolazione digitale o Etica digitale?
> La *governance digitale*, *l'etica digitale* e la *regolamentazione digitale* sono approcci normativi diversi, complementari e interconnessi, che non devono essere confusi tra loro:
> - **L‚Äôetica digitale** √® un altro approccio normativo che studia e valuta i problemi morali relativi a dati e informazioni, algoritmi e pratiche connesse, con l'obiettivo di formulare soluzioni moralmente buone. 
> 	- Influenza la *regolazione* e la *governance* attraverso la valutazione di ci√≤ che √® socialmente accettabile e preferibile.
> - **La regolazione digitale** si riferisce al sistema di leggi emanate e applicate dalle istituzioni per regolare il comportamento degli agenti nell'infosfera.
> - **La governance digitale** pu√≤ comprendere linee guida che si sovrappongono alla regolazione, ma non sono identiche ad essa.
### üèÜ Obiettivi della governance digitale
- **Politiche e procedure:** definizione di linee guida e protocolli per la gestione delle tecnologie digitali.
	- *La governance digitale include la definizione e il controllo dei processi e dei metodi utilizzati dai gestori e custodi dei dati per migliorare la qualit√†, l'affidabilit√†, l'accesso, la sicurezza e la disponibilit√† dei loro servizi.*
- **Standard:** stabilire criteri di riferimento per lo sviluppo, l'implementazione e l'uso delle tecnologie digitali.
	- *Assicura che i processi decisionali e le pratiche di gestione delle tecnologie digitali siano trasparenti e comprensibili ([[#Principio di esplicabilit√†]])*.
	- *Implementa **meccanismi di controllo** per garantire la qualit√† e l'affidabilit√† dei servizi digitali.*
- **Responsabilit√†:** identificare chiaramente i ruoli e le responsabilit√† degli attori coinvolti nella gestione delle tecnologie digitali.
	- *Protegge i sistemi digitali da minacce e vulnerabilit√†, garantendo la cyber-resilienza dell'IA*

> [!EXAMPLE] Esempi di applicazione
> - Il "[[REGOLAMENTO (UE) 2024-1689 DEL PARLAMENTO EUROPEO E DEL CONSIGLIO.pdf|Regolamento Europeo sull'IA]]" √® un complesso sistema di governance digitale, strutturato a livello Europeo e nazionale.
> - La *governance digitale* pu√≤ essere applicata per determinare e controllare i processi e i metodi utilizzati dai gestori di dati, al fine di migliorare la qualit√† e la sicurezza dei servizi.
### ‚ö†Ô∏è Sfide e criticit√†
- La *governance digitale* deve confrontarsi con la rapida evoluzione delle tecnologie e con la difficolt√† di **prevedere tutti gli impatti sociali ed etici dell'IA.**
- √à importante evitare sovrapposizioni e conflitti di competenza tra le diverse autorit√† di regolamentazione.
- La necessit√† di **standard e protocolli comuni e condivisi a livello globale**, √® fondamentale anche per garantire la libera circolazione dei prodotti.
## Etica 'soft'
L'etica soft √® un approccio normativo che si distingue dall'[[#Etica 'hard']] (nonostante siano interconnesse) per il suo campo di applicazione e per il suo rapporto con la legge. ==L'etica soft riguarda *ci√≤ che dovrebbe o non dovrebbe essere fatto* ma considerando le norme vigenti.==
- **Post-compliance:** l'etica soft si applica dopo il rispetto delle norme giuridiche, operando in uno spazio dove "il dover fare qualcosa implica il poter fare quel qualcosa". √à un'etica che va oltre il minimo richiesto dalla legge.
- **Approccio proattivo:** l'etica soft incoraggia le organizzazioni a *valutare e decidere autonomamente* quale ruolo desiderano svolgere nell'[[Infosfera]] quando le norme non forniscono una risposta semplice o diretta, creando "zone grigie".
	- *L'etica soft fornisce un quadro per interpretare i considerando e gli articoli di normative complesse come il GDPR.*

> [!NOTE] Etica soft *vs* Etica hard
> L'etica soft e l'etica hard sono **interconnesse** e spesso **si intrecciano** [1, 10]. La distinzione √® utile a livello teorico, ma nella pratica le due forme di etica agiscono spesso di pari passo [10]. 
> - L'etica hard contribuisce a **plasmare o modificare le leggi**;
> - L'etica soft si concentra su come agire in modo etico all'interno del quadro legale esistente.
# AI for Social Good (*AI4SG*)
AI4SG √® l'acronimo di "*AI for Social Good*" e si riferisce all'uso dell'IA per il bene sociale, con l'obiettivo di ==risolvere problemi che incidono negativamente sulla vita umana o sul benessere del mondo naturale e di consentire sviluppi socialmente preferibili o sostenibili dal punto di vista ambientale,== minimizzando i potenziali danni.
- Un'AI4SG ha come pre-requisito il soddisfare tutti i [[#I 5 principi fondamentali per un'IA etica|principi etici fondamentali]].

> [!EXAMPLE] Aree di applicazione
> L'AI4SG pu√≤ essere applicata in una vasta gamma di settori, tra cui:
> - **Prevenzione e mitigazione del rischio** (prevenire insuccessi accademici, attivit√† di polizia illegali, frodi aziendali)
> - Tutela paesaggistica;
> - Attivit√† di contrasto contro infrazione di copyright, deepfake, disinformazione online e individuazione responsabilit√†;
> - Studio sulla disuguaglianza economica, discriminazione sociale e monopoli;
> - Sicurezza nazionale;
> - Ottimizzazione delle risorse ambientali ed energetiche;
> - Supporto al patrimonio intellettuale e culturale digitalizzato.
## I fattori essenziali per un'AI4SG
- **Falsificabilit√† e implementazione incrementale:** i progettisti devono identificare i requisiti falsificabili e testarli in fasi incrementali, dal laboratorio al "mondo esterno". Questo significa che devono essere stabiliti dei requisiti ingegneristici che possano essere verificati e che i sistemi vengano testati progressivamente per migliorare l'affidabilit√†.
- **[[Aspetti giuridici dell'IA#Garanzie contro la manipolazione dei predittori|Garanzie contro la manipolazione dei predittori]]:** i progettisti devono adottare garanzie per evitare che i dati di input vengano manipolati e che ci sia un'eccessiva dipendenza da indicatori non causali.
- **Intervento contestualizzato in ragione del destinatario:** √® necessario adattare l'intervento al contesto specifico e alle esigenze del destinatario.
- **Spiegazione contestualizzata in ragione del destinatario e finalit√† trasparenti:** le operazioni e i risultati dei sistemi di AI4SG devono essere spiegabili e gli scopi trasparenti.
- **[[Aspetti giuridici dell'IA#Tutela della privacy e consenso dell'interessato|Tutela della privacy e consenso dell'interessato]]:** √® fondamentale proteggere la privacy e ottenere il consenso per l'utilizzo dei dati.
- **Equit√† concreta:** √® importante promuovere la parit√† di accesso ed evitare effetti discriminatori.
- **[[Aspetti giuridici dell'IA#Semantizzazione adatta all'umano|Semantizzazione adatta all'umano]]:** occorre garantire che gli esseri umani possano curare e promuovere il proprio "capitale semantico", ovvero la capacit√† di dare significato e comprendere le cose.